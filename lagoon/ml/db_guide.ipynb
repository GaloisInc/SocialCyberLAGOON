{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlalchemy as sa\n",
                "from lagoon.db.connection import get_session\n",
                "from lagoon.db import schema as sch\n",
                "from lagoon.ml.common import utils\n",
                "from lagoon.ml.config import *\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import arrow\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "sess = get_session().__enter__()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Things present in the graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1000450\n",
                        "[<lagoon.db.schema.Entity 1: EntityTypeEnum.git_commit bb3fdcfe95>, <lagoon.db.schema.Entity 2: EntityTypeEnum.git_commit a8ae7a5613>, <lagoon.db.schema.Entity 3: EntityTypeEnum.git_commit 276a3a6a16>]\n"
                    ]
                }
            ],
            "source": [
                "## Entities\n",
                "entities = sess.query(sch.Entity) #this is a query object\n",
                "print(entities.count())\n",
                "# len(entities) is invalid, however, indexing works\n",
                "print(entities[:3])\n",
                "\n",
                "# entities_list = entities.all() #the all() converts a query to a list\n",
                "# print(len(entities_list))\n",
                "# NOTE: dealing with lists takes a lot of time, so stick to queries.\n",
                "# NOTE: When iterating, i.e. `for entity in entities`, time taken is the same for a) queries and b) queries converted to lists using .all(). So, stick to queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "4051745\n"
                    ]
                }
            ],
            "source": [
                "## Observations\n",
                "observations = sess.query(sch.Observation)\n",
                "print(observations.count())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "929921\n"
                    ]
                }
            ],
            "source": [
                "## Computed attributes\n",
                "computed_attrs = sess.query(sch.ComputedAttrs)\n",
                "print(computed_attrs.count())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema.Entity'> at 10e2e45e0>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x13990be20>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'asdict': <bound method DataClassMixin.asdict of <lagoon.db.schema.Entity 1: EntityTypeEnum.git_commit bb3fdcfe95>>\n",
                        "'attrs': {'time': 1636140101.0, 'message': 'Python 3.9.8\\n', 'commit_sha': 'bb3fdcfe95b9aeed13b7201ffbc634752ad8ecc9'}\n",
                        "'batch': Batch(id=1, resource='ingest-git-github.com/python/cpython.git', ingest_time=datetime.datetime(2021, 11, 24, 21, 42, 27, 889401), revision=None)\n",
                        "'batch_id': 1\n",
                        "'computed_attrs': SELECT computed_attrs.id AS computed_attrs_id, attrs_base.id AS attrs_base_id, attrs_base.super_type AS attrs_base_super_type, attrs_base.attrs AS attrs_base_attrs, computed_attrs.batch_id AS computed_attrs_batch_id, computed_attrs.obj_id AS computed_attrs_obj_id \n",
                        "FROM attrs_base JOIN computed_attrs ON computed_attrs.id = attrs_base.id \n",
                        "WHERE %(param_1)s = computed_attrs.obj_id ORDER BY computed_attrs.id\n",
                        "'fused': <bound method Entity.fused of <lagoon.db.schema.Entity 1: EntityTypeEnum.git_commit bb3fdcfe95>>\n",
                        "'id': 1\n",
                        "'metadata': MetaData()\n",
                        "'name': bb3fdcfe95\n",
                        "'obs_as_dst': SELECT observation.id AS observation_id, attrs_base.id AS attrs_base_id, attrs_base.super_type AS attrs_base_super_type, attrs_base.attrs AS attrs_base_attrs, observation.batch_id AS observation_batch_id, observation.type AS observation_type, observation.time AS observation_time, observation.dst_id AS observation_dst_id, observation.src_id AS observation_src_id \n",
                        "FROM attrs_base JOIN observation ON attrs_base.id = observation.id \n",
                        "WHERE %(param_1)s = observation.dst_id\n",
                        "'obs_as_src': SELECT observation.id AS observation_id, attrs_base.id AS attrs_base_id, attrs_base.super_type AS attrs_base_super_type, attrs_base.attrs AS attrs_base_attrs, observation.batch_id AS observation_batch_id, observation.type AS observation_type, observation.time AS observation_time, observation.dst_id AS observation_dst_id, observation.src_id AS observation_src_id \n",
                        "FROM attrs_base JOIN observation ON attrs_base.id = observation.id \n",
                        "WHERE %(param_1)s = observation.src_id\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'super_type': SuperTypeEnum.entity\n",
                        "'type': EntityTypeEnum.git_commit\n",
                        "_______________________________________________________\n",
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema.Observation'> at 10e2fe040>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x139927c10>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'asdict': <bound method DataClassMixin.asdict of <lagoon.db.schema.Observation 152116: (ObservationTypeEnum.created@2021-11-05, <lagoon.db.schema.Entity 130903: EntityTypeEnum.person Łukasz Langa <lukasz@langa.pl>>, <lagoon.db.schema.Entity 1: EntityTypeEnum.git_commit bb3fdcfe95>)>>\n",
                        "'attrs': {}\n",
                        "'batch': Batch(id=1, resource='ingest-git-github.com/python/cpython.git', ingest_time=datetime.datetime(2021, 11, 24, 21, 42, 27, 889401), revision=None)\n",
                        "'batch_id': 1\n",
                        "'computed_attrs': SELECT computed_attrs.id AS computed_attrs_id, attrs_base.id AS attrs_base_id, attrs_base.super_type AS attrs_base_super_type, attrs_base.attrs AS attrs_base_attrs, computed_attrs.batch_id AS computed_attrs_batch_id, computed_attrs.obj_id AS computed_attrs_obj_id \n",
                        "FROM attrs_base JOIN computed_attrs ON computed_attrs.id = attrs_base.id \n",
                        "WHERE %(param_1)s = computed_attrs.obj_id ORDER BY computed_attrs.id\n",
                        "'dst': <lagoon.db.schema.Entity 1: EntityTypeEnum.git_commit bb3fdcfe95>\n",
                        "'dst_id': 1\n",
                        "'id': 152116\n",
                        "'metadata': MetaData()\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'src': <lagoon.db.schema.Entity 130903: EntityTypeEnum.person Łukasz Langa <lukasz@langa.pl>>\n",
                        "'src_id': 130903\n",
                        "'super_type': SuperTypeEnum.observation\n",
                        "'time': 2021-11-05 19:21:41\n",
                        "'type': ObservationTypeEnum.created\n",
                        "_______________________________________________________\n",
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema.ComputedAttrs'> at 10e3084a0>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1399459d0>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'asdict': <bound method DataClassMixin.asdict of ComputedAttrs(id=12983791, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={}, batch_id=29, obj_id=18)>\n",
                        "'attrs': {}\n",
                        "'batch': Batch(id=29, resource='toxicity_badwords', ingest_time=datetime.datetime(2021, 11, 25, 11, 34, 56, 461953), revision=None)\n",
                        "'batch_id': 29\n",
                        "'computed_attrs': SELECT computed_attrs.id AS computed_attrs_id, attrs_base.id AS attrs_base_id, attrs_base.super_type AS attrs_base_super_type, attrs_base.attrs AS attrs_base_attrs, computed_attrs.batch_id AS computed_attrs_batch_id, computed_attrs.obj_id AS computed_attrs_obj_id \n",
                        "FROM attrs_base JOIN computed_attrs ON computed_attrs.id = attrs_base.id \n",
                        "WHERE %(param_1)s = computed_attrs.obj_id ORDER BY computed_attrs.id\n",
                        "'id': 12983791\n",
                        "'metadata': MetaData()\n",
                        "'obj': <lagoon.db.schema.Entity 18: EntityTypeEnum.git_commit d53d9e7f4f>\n",
                        "'obj_id': 18\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'super_type': SuperTypeEnum.computed_attrs\n"
                    ]
                }
            ],
            "source": [
                "# Properties\n",
                "e = entities[0]\n",
                "for attr in dir(e):\n",
                "    if not attr.startswith('__'):\n",
                "        print(f\"'{attr}': {getattr(e,attr)}\")\n",
                "print(\"_______________________________________________________\")\n",
                "ob = observations[0]\n",
                "for attr in dir(ob):\n",
                "    if not attr.startswith('__'):\n",
                "        print(f\"'{attr}': {getattr(ob,attr)}\")\n",
                "print(\"_______________________________________________________\")\n",
                "ca = computed_attrs[0]\n",
                "for attr in dir(ca):\n",
                "    if not attr.startswith('__'):\n",
                "        print(f\"'{attr}': {getattr(ca,attr)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fused Entities and Observations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "986139\n",
                        "4051738\n"
                    ]
                }
            ],
            "source": [
                "## Get fused entities and observations\n",
                "fused_entities = sess.query(sch.FusedEntity)\n",
                "print(fused_entities.count())\n",
                "fused_observations = sess.query(sch.FusedObservation)\n",
                "print(fused_observations.count())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema_fused.FusedEntity'> at 10e325ae0>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1399cde80>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'asdict': <bound method DataClassMixin.asdict of <lagoon.db.schema_fused.FusedEntity 82710: EntityTypeEnum.git_commit dab46f7250>>\n",
                        "'attrs': {'time': 1336997840.0, 'message': 'null merge\\n', 'commit_sha': 'dab46f72501750dba0c36e26d8b6679d0d9d5f54'}\n",
                        "'attrs_sources': [<lagoon.db.schema.Entity 82710: EntityTypeEnum.git_commit dab46f7250>, ComputedAttrs(id=13054398, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={}, batch_id=29, obj_id=82710)]\n",
                        "'fusions': [EntityFusion(id_lowest=82710, id_other=82710, comment=None)]\n",
                        "'id': 82710\n",
                        "'metadata': MetaData()\n",
                        "'name': dab46f7250\n",
                        "'obs_hops': <bound method FusedEntity.obs_hops of <lagoon.db.schema_fused.FusedEntity 82710: EntityTypeEnum.git_commit dab46f7250>>\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'type': EntityTypeEnum.git_commit\n",
                        "_______________________________________________________\n",
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema_fused.FusedObservation'> at 10e32ed10>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x139a12760>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'asdict': <bound method DataClassMixin.asdict of <lagoon.db.schema_fused.FusedObservation 12920792: (ObservationTypeEnum.created@2003-01-26, 132310, 12919910)>>\n",
                        "'attrs': {}\n",
                        "'attrs_sources': [<lagoon.db.schema.Observation 12920792: (ObservationTypeEnum.created@2003-01-26, 12920420, 12919910)>]\n",
                        "'batch_id': 27\n",
                        "'dst': <lagoon.db.schema_fused.FusedEntity 12919910: EntityTypeEnum.pep PEP 305>\n",
                        "'dst_id': 12919910\n",
                        "'id': 12920792\n",
                        "'metadata': MetaData()\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10e295a60>\n",
                        "'src': <lagoon.db.schema_fused.FusedEntity 132310: EntityTypeEnum.person Dave Cole <djc@object-craft.com.au>>\n",
                        "'src_id': 132310\n",
                        "'time': 2003-01-26 00:00:00\n",
                        "'type': ObservationTypeEnum.created\n"
                    ]
                }
            ],
            "source": [
                "fe = fused_entities[0]\n",
                "for attr in dir(fe):\n",
                "    if not attr.startswith('__'):\n",
                "        try:\n",
                "            print(f\"'{attr}': {getattr(fe,attr)}\")\n",
                "        except:\n",
                "            pass\n",
                "print(\"_______________________________________________________\")\n",
                "fob = fused_observations[0]\n",
                "for attr in dir(fob):\n",
                "    if not attr.startswith('__'):\n",
                "        print(f\"'{attr}': {getattr(fob,attr)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hops"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2\n",
                        "19021\n",
                        "107004\n",
                        "<lagoon.db.schema_fused.FusedObservation 541694: (ObservationTypeEnum.created@2012-05-14, <lagoon.db.schema_fused.FusedEntity 132225: EntityTypeEnum.person Martin v. Löwis <martin@v.loewis.de>>, <lagoon.db.schema_fused.FusedEntity 82710: EntityTypeEnum.git_commit dab46f7250>)>\n"
                    ]
                }
            ],
            "source": [
                "# Get observations k hops out\n",
                "some_entity = sess.query(sch.FusedEntity).get(82710)\n",
                "for k in [1,2,3]:\n",
                "    print(len(some_entity.obs_hops(k))) #these are lists\n",
                "print(some_entity.obs_hops(1)[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "706\n"
                    ]
                }
            ],
            "source": [
                "# Restrict by time\n",
                "obs = some_entity.obs_hops(2, time_min=arrow.get('2012-01-01').datetime, time_max=arrow.get('2012-12-31').datetime)\n",
                "print(len(obs))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "5\n",
                        "5\n"
                    ]
                }
            ],
            "source": [
                "# obs_hops is a better way instead of getting sources and destinations separately\n",
                "\n",
                "commits = sess.query(sch.FusedEntity).where(sch.FusedEntity.type == sch.EntityTypeEnum.git_commit)\n",
                "commit = commits[1]\n",
                "# print(commit.obs_as_src).count() #this will NOT work since `'FusedEntity.obs_as_src' is not available due to lazy='raise'`\n",
                "# Likewise for dst\n",
                "\n",
                "# Instead, do this:\n",
                "obs_as_src = sess.query(sch.FusedObservation).where(sch.FusedObservation.src == commit)\n",
                "obs_as_dst = sess.query(sch.FusedObservation).where(sch.FusedObservation.dst == commit)\n",
                "print(obs_as_src.count() + obs_as_dst.count())\n",
                "\n",
                "# That should be equal to this\n",
                "print(len(commit.obs_hops(1)))\n",
                "\n",
                "# Since they are equivalent, it is better to just use obs_hops instead of obs_as_src and obs_as_dst"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Batches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Batch(id=1, resource='ingest-git-github.com/python/cpython.git', ingest_time=datetime.datetime(2021, 11, 24, 21, 42, 27, 889401), revision=None), Batch(id=26, resource='ocean-python.pck', ingest_time=datetime.datetime(2021, 11, 25, 7, 3, 49, 724241), revision=None), Batch(id=27, resource='ingest-python-peps', ingest_time=datetime.datetime(2021, 11, 25, 8, 15, 18, 124416), revision=None), Batch(id=28, resource='link-python-peps', ingest_time=datetime.datetime(2021, 11, 25, 11, 34, 45, 457041), revision=None), Batch(id=33, resource='toxicity_badwords', ingest_time=datetime.datetime(2021, 12, 1, 19, 32, 57, 604580), revision=None), Batch(id=50, resource='toxicity_nlp', ingest_time=datetime.datetime(2022, 2, 17, 19, 0, 51, 402608), revision=None), Batch(id=57, resource='hibp-breaches', ingest_time=datetime.datetime(2022, 2, 23, 23, 47, 57, 820284), revision=None)]\n"
                    ]
                }
            ],
            "source": [
                "batches = sess.query(sch.Batch).all()\n",
                "print(batches)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## OUTDATED\n",
                "# Get number of entities and observations of each type in each batch\n",
                "from lagoon.db.schema import EntityTypeEnum, ObservationTypeEnum\n",
                "with get_session() as sess:\n",
                "    for batch_id in [3,25]:\n",
                "        print(f'Batch id {batch_id}:')\n",
                "        for typ in EntityTypeEnum:\n",
                "            entities = sess.query(sch.FusedEntity).where(sch.FusedEntity.batch_id==batch_id).where(sch.FusedEntity.type==typ)\n",
                "            print(f'{typ}: {entities.count()}')\n",
                "        for typ in ObservationTypeEnum:\n",
                "            obs = sess.query(sch.FusedObservation).where(sch.FusedObservation.batch_id==batch_id).where(sch.FusedObservation.type==typ)\n",
                "            print(f'{typ}: {obs.count()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## OUTDATED\n",
                "# Check if there is batch overlap\n",
                "for batch_id in [3,25]:\n",
                "    print(f'Batch id {batch_id}:')\n",
                "    entity_ids = set()\n",
                "    obs = sess.query(sch.FusedObservation).where(sch.FusedObservation.batch_id==batch_id)\n",
                "    for ob in obs:\n",
                "        entity_ids.add(ob.src_id)\n",
                "        entity_ids.add(ob.dst_id)\n",
                "    for entity_id in entity_ids:\n",
                "        entity = sess.query(sch.FusedEntity).get(entity_id)\n",
                "        if entity.batch_id != batch_id:\n",
                "            print(f'Mismatch: {entity.id}')\n",
                "\n",
                "# RESULT: There is no overlap "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cpython...\n",
                        "1990-08-09 14:25:15\n",
                        "2021-11-08 16:51:01\n",
                        "OCEAN...\n",
                        "1995-03-16 06:08:16\n",
                        "2021-05-01 03:07:50\n"
                    ]
                }
            ],
            "source": [
                "# Get start and end times for observations in a batch\n",
                "# Alternatively, delete the batch portion to get this for the whole graph\n",
                "\n",
                "print('Cpython...')\n",
                "obs = (\n",
                "    sess.query(sch.FusedObservation)\n",
                "    .where(sch.FusedObservation.batch_id == 1)\n",
                ")\n",
                "times = sorted([ob.time for ob in obs])\n",
                "print(times[0])\n",
                "print(times[-1])\n",
                "\n",
                "print('OCEAN...')\n",
                "obs = (\n",
                "    sess.query(sch.FusedObservation)\n",
                "    .where(sch.FusedObservation.batch_id == 26)\n",
                ")\n",
                "times = sorted([ob.time for ob in obs])\n",
                "print(times[0])\n",
                "print(times[-1])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('badwords_ex_googleInstantB_any',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Appearance',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Generic',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Intelligence',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Politics',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Racial',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Sexual',),\n",
                            " ('badwords_ex_swearing_any',),\n",
                            " ('body_text',),\n",
                            " ('commit_sha',),\n",
                            " ('computed_badwords_googleInstantB_any',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Appearance',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Generic',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Intelligence',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Politics',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Racial',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Sexual',),\n",
                            " ('computed_badwords_swearing_any',),\n",
                            " ('created',),\n",
                            " ('email',),\n",
                            " ('message',),\n",
                            " ('name',),\n",
                            " ('number',),\n",
                            " ('origin_filename',),\n",
                            " ('replaces',),\n",
                            " ('requires',),\n",
                            " ('status',),\n",
                            " ('subject',),\n",
                            " ('superseded_by',),\n",
                            " ('time',),\n",
                            " ('title',),\n",
                            " ('type',),\n",
                            " ('url',)]"
                        ]
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# All attributes for all entities\n",
                "keys = sess.query(sa.func.jsonb_object_keys(sch.FusedEntity.attrs)).distinct()\n",
                "sorted(keys.all())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('badwords_ex_googleInstantB_any',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Appearance',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Generic',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Intelligence',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Politics',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Racial',),\n",
                            " ('badwords_ex_mrezvan94Harassment_Sexual',),\n",
                            " ('badwords_ex_swearing_any',),\n",
                            " ('body_text',),\n",
                            " ('computed_badwords_googleInstantB_any',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Appearance',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Generic',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Intelligence',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Politics',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Racial',),\n",
                            " ('computed_badwords_mrezvan94Harassment_Sexual',),\n",
                            " ('computed_badwords_swearing_any',),\n",
                            " ('origin_filename',),\n",
                            " ('subject',),\n",
                            " ('time',)]"
                        ]
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# All attributes for all message entities\n",
                "mkeys = sess.query(sa.func.jsonb_object_keys(sch.FusedEntity.attrs)).where(sch.FusedEntity.type == sch.EntityTypeEnum.message).distinct()\n",
                "sorted(mkeys.all())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 116,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[<lagoon.db.schema_fused.FusedEntity 130934: EntityTypeEnum.person Gregory P. Smith <greg@krypto.org>>, <lagoon.db.schema_fused.FusedEntity 141768: EntityTypeEnum.file Lib/bsddb/dbtables.py>]\n",
                        "\n",
                        "[<lagoon.db.schema_fused.FusedObservation 447223: (ObservationTypeEnum.modified@2007-10-18, <lagoon.db.schema_fused.FusedEntity 61516: EntityTypeEnum.git_commit f8a2a0b5a9>, <lagoon.db.schema_fused.FusedEntity 141768: EntityTypeEnum.file Lib/bsddb/dbtables.py>)>, <lagoon.db.schema_fused.FusedObservation 447221: (ObservationTypeEnum.created@2007-10-18, <lagoon.db.schema_fused.FusedEntity 130934: EntityTypeEnum.person Gregory P. Smith <greg@krypto.org>>, <lagoon.db.schema_fused.FusedEntity 61516: EntityTypeEnum.git_commit f8a2a0b5a9>)>, <lagoon.db.schema_fused.FusedObservation 447222: (ObservationTypeEnum.committed@2007-10-18, <lagoon.db.schema_fused.FusedEntity 130934: EntityTypeEnum.person Gregory P. Smith <greg@krypto.org>>, <lagoon.db.schema_fused.FusedEntity 61516: EntityTypeEnum.git_commit f8a2a0b5a9>)>]\n",
                        "\n",
                        "{'lines': 8, 'deletions': 4, 'insertions': 4}\n"
                    ]
                }
            ],
            "source": [
                "# Get number of lines changed by looking at the observations between a commit and a file\n",
                "commit = sess.query(sch.FusedEntity).get(61516)\n",
                "print(utils.get_neighboring_entities(sess, commit).all()) #this gives id of a file changed as 141768\n",
                "print()\n",
                "obs = commit.obs_hops(1) #this gives id of the obs linking to file as 447223\n",
                "print(obs)\n",
                "print()\n",
                "ob = sess.query(sch.FusedObservation).get(447223)\n",
                "print(ob.attrs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Toxicity analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if flagged_abuse is ever not None\n",
                "fused_entities = sess.query(sch.FusedEntity)\n",
                "count = 0\n",
                "for fe in fused_entities:\n",
                "    if fe.attrs.get('flagged_abuse'):\n",
                "        print(fe.attrs)\n",
                "        print(\"______________________________________\")\n",
                "        count += 1\n",
                "    if count==2:\n",
                "        break\n",
                "\n",
                "# RESULT: flagged_abuse is always None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<lagoon.db.schema_fused.FusedEntity 9801186: EntityTypeEnum.message Message <200006291349.IAA09962@cj20424-a.reston1.va.home.com>>\n",
                        "\n",
                        "[EntityFusion(id_lowest=9801186, id_other=9801186, comment=None)]\n",
                        "\n",
                        "{'time': 962286572.0, 'subject': 'Re: [Python-Dev] PyString_GET_SIZE()', 'body_text': \"> Hmm. Is there a good place to start listing these todo items? One that we\\n> can truly use for communicating this info? In Apache, we have a file named\\n> STATUS that everybody uses for dropping ideas, patch references, critical\\n> bugs, etc. As that file gets changed, we see it in the -checkins alias, so\\n> everybody is aware of the suggested changes/problems/available patches/etc.\\n> It also holds people's votes on particular changes.\\n> \\n> Can we institute something similar? Possibly Misc/STATUS? Should I post\\n> Apache's STATUS file as an example?\\n\\nPossibly, but I'm somewhat skeptical.  I used to have a large TODO\\nfile -- still have it -- but it's so full of long-term ideas that\\nnever happened that I rarely look in it any more.  I never got into\\nthe discipline of using it for my day-to-day priorities -- it was more\\nof a place to write down long-term ideas so I could forget about\\nthem.  I'll append it.  I appreciate a copy of Apache's STATUS file.\\n\\n--Guido van Rossum (home page: http://www.python.org/~guido/)\\n\\nNEW TODO LIST FOR 1.6\\n=====================\\n\\nIDLE: set window class?\\nsocket/ssl version mystery\\n\\nUNIFIED TODO LIST\\n=================\", 'origin_filename': '2000-06-mailman-python-dev.mbox.gz'}\n",
                        "\n",
                        "[<lagoon.db.schema.Entity 9801186: EntityTypeEnum.message Message <200006291349.IAA09962@cj20424-a.reston1.va.home.com>>, ComputedAttrs(id=13883339, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={}, batch_id=29, obj_id=9801186)]\n",
                        "\n",
                        "[{'time': 962286572.0, 'subject': 'Re: [Python-Dev] PyString_GET_SIZE()', 'body_text': \"> Hmm. Is there a good place to start listing these todo items? One that we\\n> can truly use for communicating this info? In Apache, we have a file named\\n> STATUS that everybody uses for dropping ideas, patch references, critical\\n> bugs, etc. As that file gets changed, we see it in the -checkins alias, so\\n> everybody is aware of the suggested changes/problems/available patches/etc.\\n> It also holds people's votes on particular changes.\\n> \\n> Can we institute something similar? Possibly Misc/STATUS? Should I post\\n> Apache's STATUS file as an example?\\n\\nPossibly, but I'm somewhat skeptical.  I used to have a large TODO\\nfile -- still have it -- but it's so full of long-term ideas that\\nnever happened that I rarely look in it any more.  I never got into\\nthe discipline of using it for my day-to-day priorities -- it was more\\nof a place to write down long-term ideas so I could forget about\\nthem.  I'll append it.  I appreciate a copy of Apache's STATUS file.\\n\\n--Guido van Rossum (home page: http://www.python.org/~guido/)\\n\\nNEW TODO LIST FOR 1.6\\n=====================\\n\\nIDLE: set window class?\\nsocket/ssl version mystery\\n\\nUNIFIED TODO LIST\\n=================\", 'flagged_abuse': None, 'origin_filename': '2000-06-mailman-python-dev.mbox.gz'}, {}]\n",
                        "______________________________________\n",
                        "<lagoon.db.schema_fused.FusedEntity 9801187: EntityTypeEnum.message Message <3F5A97F7.7080700@ocf.berkeley.edu>>\n",
                        "\n",
                        "[EntityFusion(id_lowest=9801187, id_other=9801187, comment=None)]\n",
                        "\n",
                        "{'time': 1062901751.0, 'subject': 'Re: [Python-Dev] Changing select.select to accept iterables', 'body_text': 'Tim Peters wrote:\\n\\n> [Brett, about <http://www.python.org/sf/798046>]\\n> \\n> [Guido]\\n> \\n>>I seem to recall that that code has a long history of being hairy\\n>>and full of platform-specific issues, and I\\'d rather not see it\\n>>disturbed by an unspecific desire for more generalization.  Why can\\'t\\n>>the OP produce the input in the form of lists?\\n> \\n> \\n> They could, but they specifically already have Sets of socket objects.\\n> That\\'s what C would have used too for select(), if it had sets, so it\\'s a\\n> natural desire.  The SF report notes that when read and write lists change\\n> frequently, it\\'s a lot more efficient to add/remove sockets via O(1) Set\\n> operations.  Under the covers, select() setup takes O(N) time even if the\\n> inputs are native list.\\n> \\n\\nTim is right about there not being a restriction of not being able to \\ncreate a list but wanting a better data structure for the job.\\n\\nAnd again I suspect Time is right about the socket API designers wanting \\nsets if they had it in ANSI C; probably why they call it an fd_set for \\nstoring what file descriptors to work on.\\n\\nAs for the code being hairy, yes, it does have its share of #ifdefs. \\nBut the list checks and code directly handling the lists are outside of \\nany #ifdefs.  The list codes is literally just the list checks and a \\n\\'for\\' loop pulling out each item in the list using PyList_GetItem; after \\nthat everything operates on what was pulled out of the list.  But I \\nunderstand the reluctance of messing with code that has a reputation and \\nhis a pain to test for something that is just a perk.\\n\\n> Functions that expect input lists of reasonably small size (thousands, sure;\\n> millions, probably not) can usually be generalized to iterables easily, with\\n> little chance of disruption, by replacing initial \"is it a list?\" check with\\n> a call to PySequence_Fast(), then\\n> s/PyList_GetItem/PySequence_Fast_GET_ITEM/.\\n> \\n\\nIs this the preferred way of dealing with places where a small sequence \\nand such will be instead of bothering with iterators, or is this a \\nhold-over from pre-iterators and thus iterators are the proper way to go \\nin new code and in old code where reasonable and relatively painless?\\n\\nSince Guido has previously expressed reluctance in having me touch this \\ncode I won\\'t unless he tells me otherwise.\\n\\n-Brett', 'origin_filename': '2003-09-mailman-python-dev.mbox.gz', 'badwords_ex_googleInstantB_any': ['hairy'], 'computed_badwords_googleInstantB_any': 2}\n",
                        "\n",
                        "[<lagoon.db.schema.Entity 9801187: EntityTypeEnum.message Message <3F5A97F7.7080700@ocf.berkeley.edu>>, ComputedAttrs(id=13611848, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={'badwords_ex_googleInstantB_any': ['hairy'], 'computed_badwords_googleInstantB_any': 2}, batch_id=29, obj_id=9801187)]\n",
                        "\n",
                        "[{'time': 1062901751.0, 'subject': 'Re: [Python-Dev] Changing select.select to accept iterables', 'body_text': 'Tim Peters wrote:\\n\\n> [Brett, about <http://www.python.org/sf/798046>]\\n> \\n> [Guido]\\n> \\n>>I seem to recall that that code has a long history of being hairy\\n>>and full of platform-specific issues, and I\\'d rather not see it\\n>>disturbed by an unspecific desire for more generalization.  Why can\\'t\\n>>the OP produce the input in the form of lists?\\n> \\n> \\n> They could, but they specifically already have Sets of socket objects.\\n> That\\'s what C would have used too for select(), if it had sets, so it\\'s a\\n> natural desire.  The SF report notes that when read and write lists change\\n> frequently, it\\'s a lot more efficient to add/remove sockets via O(1) Set\\n> operations.  Under the covers, select() setup takes O(N) time even if the\\n> inputs are native list.\\n> \\n\\nTim is right about there not being a restriction of not being able to \\ncreate a list but wanting a better data structure for the job.\\n\\nAnd again I suspect Time is right about the socket API designers wanting \\nsets if they had it in ANSI C; probably why they call it an fd_set for \\nstoring what file descriptors to work on.\\n\\nAs for the code being hairy, yes, it does have its share of #ifdefs. \\nBut the list checks and code directly handling the lists are outside of \\nany #ifdefs.  The list codes is literally just the list checks and a \\n\\'for\\' loop pulling out each item in the list using PyList_GetItem; after \\nthat everything operates on what was pulled out of the list.  But I \\nunderstand the reluctance of messing with code that has a reputation and \\nhis a pain to test for something that is just a perk.\\n\\n> Functions that expect input lists of reasonably small size (thousands, sure;\\n> millions, probably not) can usually be generalized to iterables easily, with\\n> little chance of disruption, by replacing initial \"is it a list?\" check with\\n> a call to PySequence_Fast(), then\\n> s/PyList_GetItem/PySequence_Fast_GET_ITEM/.\\n> \\n\\nIs this the preferred way of dealing with places where a small sequence \\nand such will be instead of bothering with iterators, or is this a \\nhold-over from pre-iterators and thus iterators are the proper way to go \\nin new code and in old code where reasonable and relatively painless?\\n\\nSince Guido has previously expressed reluctance in having me touch this \\ncode I won\\'t unless he tells me otherwise.\\n\\n-Brett', 'flagged_abuse': None, 'origin_filename': '2003-09-mailman-python-dev.mbox.gz'}, {'badwords_ex_googleInstantB_any': ['hairy'], 'computed_badwords_googleInstantB_any': 2}]\n"
                    ]
                }
            ],
            "source": [
                "messages = sess.query(sch.FusedEntity).where(sch.FusedEntity.type==sch.EntityTypeEnum.message)\n",
                "\n",
                "# No toxicity\n",
                "message = sess.query(sch.FusedEntity).get(9801186)\n",
                "print(message)\n",
                "print()\n",
                "print(message.fusions)\n",
                "print()\n",
                "print(message.attrs)\n",
                "print()\n",
                "print(message.attrs_sources)\n",
                "print()\n",
                "print([elem.attrs for elem in message.attrs_sources])\n",
                "\n",
                "print(\"______________________________________\")\n",
                "\n",
                "# Toxicity present\n",
                "message = sess.query(sch.FusedEntity).get(9801187)\n",
                "print(message)\n",
                "print()\n",
                "print(message.fusions)\n",
                "print()\n",
                "print(message.attrs)\n",
                "print()\n",
                "print(message.attrs_sources)\n",
                "print()\n",
                "print([elem.attrs for elem in message.attrs_sources])\n",
                "\n",
                "#NOTE: The first element of `attrs_sources` contains details of attrs at the time of creation of the FusedEntity, like 'subject' and 'body_text' for messages. The subsequent elements have attrs that were added later, like badwords counts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ComputedAttrs(id=13883339, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={}, batch_id=29, obj_id=9801186)\n",
                        "ComputedAttrs(id=13611848, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={'badwords_ex_googleInstantB_any': ['hairy'], 'computed_badwords_googleInstantB_any': 2}, batch_id=29, obj_id=9801187)\n"
                    ]
                }
            ],
            "source": [
                "# Look at the ComputedAttrs objects\n",
                "\n",
                "# Blank\n",
                "ca_blank = sess.query(sch.ComputedAttrs).get(13883339)\n",
                "print(ca_blank)\n",
                "\n",
                "# Not blank\n",
                "ca_toxic = sess.query(sch.ComputedAttrs).get(13611848)\n",
                "print(ca_toxic)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<lagoon.db.schema_fused.FusedEntity 61516: EntityTypeEnum.git_commit f8a2a0b5a9>\n",
                        "\n",
                        "[EntityFusion(id_lowest=61516, id_other=61516, comment=None)]\n",
                        "\n",
                        "{'time': 1192696460.0, 'message': 'Fix a weird bug in dbtables: if it chose a random rowid string that contained\\nNULL bytes it would cause the database all sorts of problems in the future\\nleading to very strange random failures and corrupt dbtables.bsdTableDb dbs.\\n', 'commit_sha': 'f8a2a0b5a9edc5769b2da40c36c49eed4c5c1b33'}\n",
                        "\n",
                        "[<lagoon.db.schema.Entity 61516: EntityTypeEnum.git_commit f8a2a0b5a9>, ComputedAttrs(id=13791559, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={}, batch_id=29, obj_id=61516)]\n",
                        "\n",
                        "[{'time': 1192696460.0, 'message': 'Fix a weird bug in dbtables: if it chose a random rowid string that contained\\nNULL bytes it would cause the database all sorts of problems in the future\\nleading to very strange random failures and corrupt dbtables.bsdTableDb dbs.\\n', 'commit_sha': 'f8a2a0b5a9edc5769b2da40c36c49eed4c5c1b33'}, {}]\n",
                        "______________________________________\n",
                        "<lagoon.db.schema_fused.FusedEntity 17770: EntityTypeEnum.git_commit b5a755e46c>\n",
                        "\n",
                        "[EntityFusion(id_lowest=17770, id_other=17770, comment=None)]\n",
                        "\n",
                        "{'time': 1184782548.0, 'message': 'Merged revisions 56301-56442 via svnmerge from\\nsvn+ssh://pythondev@svn.python.org/python/branches/p3yk\\n\\n................\\n  r56322 | kurt.kaiser | 2007-07-12 11:35:03 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n  Replace obscure code which was failing in py3k.\\n................\\n  r56323 | kurt.kaiser | 2007-07-12 11:44:12 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n  1. Remove new division option\\n  2. Update IDLE version to 3.0x\\n................\\n  r56327 | kurt.kaiser | 2007-07-12 12:20:20 -0700 (Thu, 12 Jul 2007) | 5 lines\\n\\n  Fix another instance of this defect in Tkinter, and one in IDLE.\\n\\n  Patch 1724999 by Ali Gholami Rudi -- avoid complaints about dict size\\n  change during iter in destroy call.\\n................\\n  r56339 | georg.brandl | 2007-07-13 03:07:25 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  Fix #1753310: regrtest -x doesn\\'t work anymore\\n................\\n  r56361 | kurt.kaiser | 2007-07-13 18:25:24 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  convert a map() iterator to a list to get this working.\\n................\\n  r56362 | kurt.kaiser | 2007-07-13 18:53:45 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  Was modifying dict during iteration.\\n................\\n  r56376 | collin.winter | 2007-07-14 11:56:19 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n  Add an example of class decorators to test_grammar.\\n................\\n  r56377 | collin.winter | 2007-07-14 12:00:17 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n  Add a basic example of dictcomps to test_grammar.\\n................\\n  r56413 | neal.norwitz | 2007-07-17 00:21:18 -0700 (Tue, 17 Jul 2007) | 149 lines\\n\\n  Merged revisions 56202-56412 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r56204 | ronald.oussoren | 2007-07-08 23:02:21 -0700 (Sun, 08 Jul 2007) | 2 lines\\n\\n    Patch 1693258: Fix for duplicate \"preferences\" menu-OS X\\n  ........\\n    r56207 | ronald.oussoren | 2007-07-09 01:41:15 -0700 (Mon, 09 Jul 2007) | 4 lines\\n\\n    Patch 1673122: be explicit about which libtool to use, to avoid name clashes\\n    when a users install GNU libtool early in his PATH\\n  ........\\n    r56280 | georg.brandl | 2007-07-11 12:41:49 -0700 (Wed, 11 Jul 2007) | 2 lines\\n\\n    Fix #1752132: wrong comment in opcode description.\\n  ........\\n    r56293 | georg.brandl | 2007-07-12 01:05:45 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Patch #1752270, #1750931: complain if urllib2 add_handler called\\n    without handler.\\n  ........\\n    r56296 | georg.brandl | 2007-07-12 01:11:29 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1739696: use code.co_code only if really necessary\\n  ........\\n    r56298 | georg.brandl | 2007-07-12 01:38:00 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Patch #1673759: add a missing overflow check when formatting floats\\n    with %G.\\n  ........\\n    r56302 | georg.brandl | 2007-07-12 02:06:41 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1731659: improve time.strptime docs.\\n  ........\\n    r56304 | georg.brandl | 2007-07-12 02:24:04 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1731169: clean up expected skips list.\\n  ........\\n    r56306 | georg.brandl | 2007-07-12 02:37:49 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Bug #1637365: add subsection about \"__name__ == __main__\" to the\\n    Python tutorial.\\n  ........\\n    r56308 | georg.brandl | 2007-07-12 02:59:22 -0700 (Thu, 12 Jul 2007) | 5 lines\\n\\n    Patch #1675424: Added tests for uncovered code in the zipfile module.\\n      The KeyError raised by Zipfile.getinfo for nonexistent names now has\\n      a descriptive message.\\n  ........\\n    r56340 | facundo.batista | 2007-07-13 03:43:44 -0700 (Fri, 13 Jul 2007) | 6 lines\\n\\n\\n    Added tests for basic behavior of DateTime, Binary, and Fault classes\\n    and the escape function. Check that marshalling recursive sequences &\\n    dicts raises TypeError. Check that marshalling out-of-range ints\\n    raises OverflowError [Alan McIntyre - GSoC]\\n  ........\\n    r56345 | raymond.hettinger | 2007-07-13 05:09:41 -0700 (Fri, 13 Jul 2007) | 1 line\\n\\n    Correct the docs for takewhile().  Improve the recipe for nth().  Should be backported\\n  ........\\n    r56348 | thomas.heller | 2007-07-13 06:59:39 -0700 (Fri, 13 Jul 2007) | 4 lines\\n\\n    Repair COMError.  Since exceptions are new style classes now, setting\\n    the methods and docstring after the type creation does not work, they\\n    must be in the dictionary before creating the type.\\n  ........\\n    r56349 | thomas.heller | 2007-07-13 07:18:06 -0700 (Fri, 13 Jul 2007) | 1 line\\n\\n    Add tests for _ctypes.COMError.\\n  ........\\n    r56350 | thomas.heller | 2007-07-13 09:50:43 -0700 (Fri, 13 Jul 2007) | 4 lines\\n\\n    Do not try to load the GLUT library in the ctypes tests.  This test\\n    adds little value, but has a large problem on OS X, as explained in\\n    SF# 1581906.\\n  ........\\n    r56352 | thomas.heller | 2007-07-13 10:12:23 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    Fix for SF# 1701409: segfault in c_char_p of ctypes.  The repr output\\n    of c_char_p and c_wchar_p has changed as a sideeffect.\\n  ........\\n    r56355 | thomas.heller | 2007-07-13 10:46:54 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    Fix for SF# 1649098: avoid zero-sized array declaration in structure.\\n  ........\\n    r56357 | thomas.heller | 2007-07-13 12:51:55 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    PyType_stgdict() returns a borrowed reference which must not be\\n    Py_DECREF\\'d.\\n  ........\\n    r56360 | barry.warsaw | 2007-07-13 15:12:58 -0700 (Fri, 13 Jul 2007) | 10 lines\\n\\n    In response to this SF bug:\\n\\n    [ 1752723 ] email.message_from_string: initial line gets discarded\\n\\n    I added a test to assert that when the first line of text passed to\\n    message_from_string() contains a leading space, the message ends up with the\\n    appropriate FirstHeaderLineIsContinuationDefect on its defects list.\\n\\n    The bug is invalid.\\n  ........\\n    r56364 | georg.brandl | 2007-07-14 10:12:23 -0700 (Sat, 14 Jul 2007) | 2 lines\\n\\n    Bug #1753406: missing \\\\versionadded for subprocess.check_call.\\n  ........\\n    r56366 | georg.brandl | 2007-07-14 10:32:41 -0700 (Sat, 14 Jul 2007) | 2 lines\\n\\n    Clarify webbrowser.open description.\\n  ........\\n    r56380 | andrew.kuchling | 2007-07-14 13:58:21 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n    Typo fix\\n  ........\\n    r56382 | andrew.kuchling | 2007-07-14 14:56:19 -0700 (Sat, 14 Jul 2007) | 7 lines\\n\\n    Avoid exception if there\\'s a stray directory inside a Maildir folder.\\n\\n    The Maildir specification doesn\\'t seem to say anything about this\\n    situation, and it can happen if you\\'re keeping a Maildir mailbox in\\n    Subversion (.svn directories) or some similar system.  The patch just\\n    ignores directories in the cur/, new/, tmp/ folders.\\n  ........\\n    r56392 | facundo.batista | 2007-07-14 15:41:45 -0700 (Sat, 14 Jul 2007) | 6 lines\\n\\n\\n    First version.  Includes tests for helper functions: read, write,\\n    _exception, readwrite, closeall, compact_traceback; and for classes\\n    dispatcher, dispatcher_with_send, and file_wrapper.\\n    [Alan McIntyre - GSoC]\\n  ........\\n    r56399 | facundo.batista | 2007-07-15 13:30:39 -0700 (Sun, 15 Jul 2007) | 5 lines\\n\\n\\n    Changed the used port and commented out some tests that uses\\n    a non documented function that appers to uses resources\\n    not present in Windows.\\n  ........\\n    r56412 | facundo.batista | 2007-07-16 19:19:39 -0700 (Mon, 16 Jul 2007) | 6 lines\\n\\n\\n    Prevent asyncore.dispatcher tests from hanging by adding loop counters\\n    to server & client, and by adding asyncore.close_all calls in\\n    tearDown. Also choose correct expected logging results based on the\\n    value of __debug__  [Alan McIntyre - GSoC]\\n  ........\\n................\\n  r56442 | guido.van.rossum | 2007-07-18 10:26:38 -0700 (Wed, 18 Jul 2007) | 14 lines\\n\\n  Merged revisions 56413-56441 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r56439 | georg.brandl | 2007-07-17 23:37:55 -0700 (Tue, 17 Jul 2007) | 2 lines\\n\\n    Use \"Unix\" as platform name, not \"UNIX\".\\n  ........\\n    r56441 | guido.van.rossum | 2007-07-18 10:19:14 -0700 (Wed, 18 Jul 2007) | 3 lines\\n\\n    SF patch# 1755885 by Kurt Kaiser: show location of Unicode escape errors.\\n    (Slightly tweaked for style and refcounts.)\\n  ........\\n................\\n', 'commit_sha': 'b5a755e46c66c7e7d709431399f1781ea9e582df', 'badwords_ex_mrezvan94Harassment_Sexual': ['destroy'], 'computed_badwords_mrezvan94Harassment_Sexual': 1}\n",
                        "\n",
                        "[<lagoon.db.schema.Entity 17770: EntityTypeEnum.git_commit b5a755e46c>, ComputedAttrs(id=13566219, super_type=<SuperTypeEnum.computed_attrs: 'computed_attrs'>, attrs={'badwords_ex_mrezvan94Harassment_Sexual': ['destroy'], 'computed_badwords_mrezvan94Harassment_Sexual': 1}, batch_id=29, obj_id=17770)]\n",
                        "\n",
                        "[{'time': 1184782548.0, 'message': 'Merged revisions 56301-56442 via svnmerge from\\nsvn+ssh://pythondev@svn.python.org/python/branches/p3yk\\n\\n................\\n  r56322 | kurt.kaiser | 2007-07-12 11:35:03 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n  Replace obscure code which was failing in py3k.\\n................\\n  r56323 | kurt.kaiser | 2007-07-12 11:44:12 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n  1. Remove new division option\\n  2. Update IDLE version to 3.0x\\n................\\n  r56327 | kurt.kaiser | 2007-07-12 12:20:20 -0700 (Thu, 12 Jul 2007) | 5 lines\\n\\n  Fix another instance of this defect in Tkinter, and one in IDLE.\\n\\n  Patch 1724999 by Ali Gholami Rudi -- avoid complaints about dict size\\n  change during iter in destroy call.\\n................\\n  r56339 | georg.brandl | 2007-07-13 03:07:25 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  Fix #1753310: regrtest -x doesn\\'t work anymore\\n................\\n  r56361 | kurt.kaiser | 2007-07-13 18:25:24 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  convert a map() iterator to a list to get this working.\\n................\\n  r56362 | kurt.kaiser | 2007-07-13 18:53:45 -0700 (Fri, 13 Jul 2007) | 2 lines\\n\\n  Was modifying dict during iteration.\\n................\\n  r56376 | collin.winter | 2007-07-14 11:56:19 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n  Add an example of class decorators to test_grammar.\\n................\\n  r56377 | collin.winter | 2007-07-14 12:00:17 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n  Add a basic example of dictcomps to test_grammar.\\n................\\n  r56413 | neal.norwitz | 2007-07-17 00:21:18 -0700 (Tue, 17 Jul 2007) | 149 lines\\n\\n  Merged revisions 56202-56412 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r56204 | ronald.oussoren | 2007-07-08 23:02:21 -0700 (Sun, 08 Jul 2007) | 2 lines\\n\\n    Patch 1693258: Fix for duplicate \"preferences\" menu-OS X\\n  ........\\n    r56207 | ronald.oussoren | 2007-07-09 01:41:15 -0700 (Mon, 09 Jul 2007) | 4 lines\\n\\n    Patch 1673122: be explicit about which libtool to use, to avoid name clashes\\n    when a users install GNU libtool early in his PATH\\n  ........\\n    r56280 | georg.brandl | 2007-07-11 12:41:49 -0700 (Wed, 11 Jul 2007) | 2 lines\\n\\n    Fix #1752132: wrong comment in opcode description.\\n  ........\\n    r56293 | georg.brandl | 2007-07-12 01:05:45 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Patch #1752270, #1750931: complain if urllib2 add_handler called\\n    without handler.\\n  ........\\n    r56296 | georg.brandl | 2007-07-12 01:11:29 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1739696: use code.co_code only if really necessary\\n  ........\\n    r56298 | georg.brandl | 2007-07-12 01:38:00 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Patch #1673759: add a missing overflow check when formatting floats\\n    with %G.\\n  ........\\n    r56302 | georg.brandl | 2007-07-12 02:06:41 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1731659: improve time.strptime docs.\\n  ........\\n    r56304 | georg.brandl | 2007-07-12 02:24:04 -0700 (Thu, 12 Jul 2007) | 2 lines\\n\\n    Patch #1731169: clean up expected skips list.\\n  ........\\n    r56306 | georg.brandl | 2007-07-12 02:37:49 -0700 (Thu, 12 Jul 2007) | 3 lines\\n\\n    Bug #1637365: add subsection about \"__name__ == __main__\" to the\\n    Python tutorial.\\n  ........\\n    r56308 | georg.brandl | 2007-07-12 02:59:22 -0700 (Thu, 12 Jul 2007) | 5 lines\\n\\n    Patch #1675424: Added tests for uncovered code in the zipfile module.\\n      The KeyError raised by Zipfile.getinfo for nonexistent names now has\\n      a descriptive message.\\n  ........\\n    r56340 | facundo.batista | 2007-07-13 03:43:44 -0700 (Fri, 13 Jul 2007) | 6 lines\\n\\n\\n    Added tests for basic behavior of DateTime, Binary, and Fault classes\\n    and the escape function. Check that marshalling recursive sequences &\\n    dicts raises TypeError. Check that marshalling out-of-range ints\\n    raises OverflowError [Alan McIntyre - GSoC]\\n  ........\\n    r56345 | raymond.hettinger | 2007-07-13 05:09:41 -0700 (Fri, 13 Jul 2007) | 1 line\\n\\n    Correct the docs for takewhile().  Improve the recipe for nth().  Should be backported\\n  ........\\n    r56348 | thomas.heller | 2007-07-13 06:59:39 -0700 (Fri, 13 Jul 2007) | 4 lines\\n\\n    Repair COMError.  Since exceptions are new style classes now, setting\\n    the methods and docstring after the type creation does not work, they\\n    must be in the dictionary before creating the type.\\n  ........\\n    r56349 | thomas.heller | 2007-07-13 07:18:06 -0700 (Fri, 13 Jul 2007) | 1 line\\n\\n    Add tests for _ctypes.COMError.\\n  ........\\n    r56350 | thomas.heller | 2007-07-13 09:50:43 -0700 (Fri, 13 Jul 2007) | 4 lines\\n\\n    Do not try to load the GLUT library in the ctypes tests.  This test\\n    adds little value, but has a large problem on OS X, as explained in\\n    SF# 1581906.\\n  ........\\n    r56352 | thomas.heller | 2007-07-13 10:12:23 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    Fix for SF# 1701409: segfault in c_char_p of ctypes.  The repr output\\n    of c_char_p and c_wchar_p has changed as a sideeffect.\\n  ........\\n    r56355 | thomas.heller | 2007-07-13 10:46:54 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    Fix for SF# 1649098: avoid zero-sized array declaration in structure.\\n  ........\\n    r56357 | thomas.heller | 2007-07-13 12:51:55 -0700 (Fri, 13 Jul 2007) | 3 lines\\n\\n    PyType_stgdict() returns a borrowed reference which must not be\\n    Py_DECREF\\'d.\\n  ........\\n    r56360 | barry.warsaw | 2007-07-13 15:12:58 -0700 (Fri, 13 Jul 2007) | 10 lines\\n\\n    In response to this SF bug:\\n\\n    [ 1752723 ] email.message_from_string: initial line gets discarded\\n\\n    I added a test to assert that when the first line of text passed to\\n    message_from_string() contains a leading space, the message ends up with the\\n    appropriate FirstHeaderLineIsContinuationDefect on its defects list.\\n\\n    The bug is invalid.\\n  ........\\n    r56364 | georg.brandl | 2007-07-14 10:12:23 -0700 (Sat, 14 Jul 2007) | 2 lines\\n\\n    Bug #1753406: missing \\\\versionadded for subprocess.check_call.\\n  ........\\n    r56366 | georg.brandl | 2007-07-14 10:32:41 -0700 (Sat, 14 Jul 2007) | 2 lines\\n\\n    Clarify webbrowser.open description.\\n  ........\\n    r56380 | andrew.kuchling | 2007-07-14 13:58:21 -0700 (Sat, 14 Jul 2007) | 1 line\\n\\n    Typo fix\\n  ........\\n    r56382 | andrew.kuchling | 2007-07-14 14:56:19 -0700 (Sat, 14 Jul 2007) | 7 lines\\n\\n    Avoid exception if there\\'s a stray directory inside a Maildir folder.\\n\\n    The Maildir specification doesn\\'t seem to say anything about this\\n    situation, and it can happen if you\\'re keeping a Maildir mailbox in\\n    Subversion (.svn directories) or some similar system.  The patch just\\n    ignores directories in the cur/, new/, tmp/ folders.\\n  ........\\n    r56392 | facundo.batista | 2007-07-14 15:41:45 -0700 (Sat, 14 Jul 2007) | 6 lines\\n\\n\\n    First version.  Includes tests for helper functions: read, write,\\n    _exception, readwrite, closeall, compact_traceback; and for classes\\n    dispatcher, dispatcher_with_send, and file_wrapper.\\n    [Alan McIntyre - GSoC]\\n  ........\\n    r56399 | facundo.batista | 2007-07-15 13:30:39 -0700 (Sun, 15 Jul 2007) | 5 lines\\n\\n\\n    Changed the used port and commented out some tests that uses\\n    a non documented function that appers to uses resources\\n    not present in Windows.\\n  ........\\n    r56412 | facundo.batista | 2007-07-16 19:19:39 -0700 (Mon, 16 Jul 2007) | 6 lines\\n\\n\\n    Prevent asyncore.dispatcher tests from hanging by adding loop counters\\n    to server & client, and by adding asyncore.close_all calls in\\n    tearDown. Also choose correct expected logging results based on the\\n    value of __debug__  [Alan McIntyre - GSoC]\\n  ........\\n................\\n  r56442 | guido.van.rossum | 2007-07-18 10:26:38 -0700 (Wed, 18 Jul 2007) | 14 lines\\n\\n  Merged revisions 56413-56441 via svnmerge from\\n  svn+ssh://pythondev@svn.python.org/python/trunk\\n\\n  ........\\n    r56439 | georg.brandl | 2007-07-17 23:37:55 -0700 (Tue, 17 Jul 2007) | 2 lines\\n\\n    Use \"Unix\" as platform name, not \"UNIX\".\\n  ........\\n    r56441 | guido.van.rossum | 2007-07-18 10:19:14 -0700 (Wed, 18 Jul 2007) | 3 lines\\n\\n    SF patch# 1755885 by Kurt Kaiser: show location of Unicode escape errors.\\n    (Slightly tweaked for style and refcounts.)\\n  ........\\n................\\n', 'commit_sha': 'b5a755e46c66c7e7d709431399f1781ea9e582df'}, {'badwords_ex_mrezvan94Harassment_Sexual': ['destroy'], 'computed_badwords_mrezvan94Harassment_Sexual': 1}]\n"
                    ]
                }
            ],
            "source": [
                "commits = sess.query(sch.FusedEntity).where(sch.FusedEntity.type==sch.EntityTypeEnum.git_commit)\n",
                "\n",
                "# No toxicity\n",
                "commit = commits[0]\n",
                "print(commit)\n",
                "print()\n",
                "print(commit.fusions)\n",
                "print()\n",
                "print(commit.attrs)\n",
                "print()\n",
                "print(commit.attrs_sources)\n",
                "print()\n",
                "print([elem.attrs for elem in commit.attrs_sources])\n",
                "\n",
                "print(\"______________________________________\")\n",
                "\n",
                "# Toxicity present\n",
                "for commit in commits:\n",
                "    if commit.attrs_sources[1].attrs:\n",
                "        print(commit)\n",
                "        print()\n",
                "        print(commit.fusions)\n",
                "        print()\n",
                "        print(commit.attrs)\n",
                "        print()\n",
                "        print(commit.attrs_sources)\n",
                "        print()\n",
                "        print([elem.attrs for elem in commit.attrs_sources])\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PEPs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First check which mailing lists are included in OCEAN\n",
                "messages = sess.query(sa.func.jsonb_extract_path_text(sch.FusedEntity.attrs,'origin_filename')).where(sch.FusedEntity.type == sch.EntityTypeEnum.message).distinct()\n",
                "print([message[0] for message in messages.all()])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "570"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "peps = sess.query(sch.FusedEntity).where(sch.FusedEntity.type==sch.EntityTypeEnum.pep)\n",
                "peps.count()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'_sa_class_manager': <ClassManager of <class 'lagoon.db.schema_fused.FusedEntity'> at 10eac65e0>\n",
                        "'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x141052d30>\n",
                        "'_sa_registry': <sqlalchemy.orm.decl_api.registry object at 0x10dcb9dc0>\n",
                        "'asdict': <bound method DataClassMixin.asdict of <lagoon.db.schema_fused.FusedEntity 12919798: EntityTypeEnum.pep PEP 13>>\n",
                        "'attrs': {'url': 'https://www.python.org/dev/peps/pep-0013', 'type': 'Informational', 'title': 'Python Language Governance', 'number': 13, 'status': 'Active', 'created': '16-Dec-2018', 'replaces': [], 'requires': [], 'superseded_by': []}\n",
                        "'attrs_sources': [<lagoon.db.schema.Entity 12919798: EntityTypeEnum.pep PEP 13>]\n",
                        "'fusions': [EntityFusion(id_lowest=12919798, id_other=12919798, comment=None)]\n",
                        "'id': 12919798\n",
                        "'metadata': MetaData()\n",
                        "'name': PEP 13\n",
                        "'obs_hops': <bound method FusedEntity.obs_hops of <lagoon.db.schema_fused.FusedEntity 12919798: EntityTypeEnum.pep PEP 13>>\n",
                        "'registry': <sqlalchemy.orm.decl_api.registry object at 0x10dcb9dc0>\n",
                        "'type': EntityTypeEnum.pep\n"
                    ]
                }
            ],
            "source": [
                "# Properties of a PEP\n",
                "pep = peps.first()\n",
                "for attr in dir(pep):\n",
                "    if not attr.startswith('__'):\n",
                "        try:\n",
                "            print(f\"'{attr}': {getattr(pep,attr)}\")\n",
                "        except:\n",
                "            pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Good = 335, Bad = 203, Other = 32\n"
                    ]
                }
            ],
            "source": [
                "# How many good/bad/other PEPs are there?\n",
                "good = 0\n",
                "bad = 0\n",
                "other = 0\n",
                "for pep in peps:\n",
                "    if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['good'])):\n",
                "        good += 1\n",
                "    if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['bad'])):\n",
                "        bad += 1\n",
                "    if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['other'])):\n",
                "        other += 1\n",
                "print(f'Good = {good}, Bad = {bad}, Other = {other}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Beginning of time to end of 2015: Good = 236, Bad = 158, Other = 3\n",
                        "2016-2020: Good = 91, Bad = 42, Other = 17\n"
                    ]
                }
            ],
            "source": [
                "# How many good/bad/other PEPs are there in the last 5 years and the time before that?\n",
                "good_old, good_new = 0, 0\n",
                "bad_old, bad_new = 0, 0\n",
                "other_old, other_new = 0, 0\n",
                "for pep in peps:\n",
                "    if int(pep.attrs['created'][7:11]) < 2016:\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['good'])):\n",
                "            good_old += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['bad'])):\n",
                "            bad_old += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['other'])):\n",
                "            other_old += 1\n",
                "    elif 2016 <= int(pep.attrs['created'][7:11]) <= 2020:\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['good'])):\n",
                "            good_new += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['bad'])):\n",
                "            bad_new += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['other'])):\n",
                "            other_new += 1\n",
                "print(f'Beginning of time to end of 2015: Good = {good_old}, Bad = {bad_old}, Other = {other_old}')\n",
                "print(f'2016-2020: Good = {good_new}, Bad = {bad_new}, Other = {other_new}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Good = 104, Bad = 44, Other = 11\n"
                    ]
                }
            ],
            "source": [
                "# How many good/bad/other multi-author PEPs are there?\n",
                "good = 0\n",
                "bad = 0\n",
                "other = 0\n",
                "for pep in peps:\n",
                "    authors = utils.get_pep_authors(pep)\n",
                "    if len(authors)>1:\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['good'])):\n",
                "            good += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['bad'])):\n",
                "            bad += 1\n",
                "        if pep.attrs['status'].lower().startswith(tuple(PEP_STATUSES['other'])):\n",
                "            other += 1\n",
                "print(f'Good = {good}, Bad = {bad}, Other = {other}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## NLP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'input_ids': tensor([[  101,  8790,   117,  1150,  1132,  1128,   136,   102,     0,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0],\n",
                        "        [  101, 11750,   106,   106,   157,  3048,  6258, 19432,  1177,   144,\n",
                        "         10460,   111,  1363,   131,   118,   114,   119,   138,   117,   139,\n",
                        "           117,  1105,   146,  1132, 16603,   119,   102],\n",
                        "        [  101, 17774,  1162,   102,     0,     0,     0,     0,     0,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
                        "             0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "         0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "         0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "         0, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "         1, 1, 1],\n",
                        "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "         0, 0, 0]])}\n",
                        "[CLS] Hi, who are you? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
                        "[CLS] Wow!! THIS IS so GRT & good : - ). A, B, and I are amazed. [SEP]\n",
                        "[CLS] Byee [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
                        "__________________\n",
                        "['Wow', '!', '!', 'T', '##H', '##IS', 'IS', 'so', 'G', '##RT', '&', 'good', ':', '-', ')', '.', 'A', ',', 'B', ',', 'and', 'I', 'are', 'amazed', '.']\n",
                        "['Hi', ',', 'who', 'are', 'you', '?', 'I', 'am', 'Op', '##ti', '##mus', 'Prime', '.']\n",
                        "__________________\n",
                        "{'input_ids': tensor([[ 101, 8790,  117, 1150, 1132, 1128,  136,  102,  146, 1821, 9126, 3121,\n",
                        "         6308, 3460,  119,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
                        "[CLS] Hi, who are you? [SEP] I am Optimus Prime. [SEP]\n"
                    ]
                }
            ],
            "source": [
                "# Basics\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
                "\n",
                "# Explanation of the tokenizer output:\n",
                "## 'input_ids' are the numerical representations of the tokens\n",
                "## 'token_type_ids' is all zeros for single sentences, or zeros and ones for pairs of sentences\n",
                "## 'attention_mask' is all ones except for unimportant tokens like [PAD], for which it is zero\n",
                "\n",
                "# Deal with a list of sentences\n",
                "sentences = [\"Hi, who are you?\", \"Wow!! THIS IS so GRT & good :-). A, B, and I are amazed.\", \"Byee\"]\n",
                "encoded_inputs = tokenizer(sentences, truncation=True, padding=True, return_tensors = 'pt')\n",
                "print(encoded_inputs)\n",
                "for single_input_ids in encoded_inputs['input_ids']:\n",
                "    decoded_input = tokenizer.decode(single_input_ids)\n",
                "    print(decoded_input)\n",
                "\n",
                "print(\"__________________\")\n",
                "\n",
                "# See tokens for a single sentence - it splits punctuation, and splits weird words (including words in caps) it can't understand by pre-pending ##\n",
                "tokenized_sequence = tokenizer.tokenize(\"Wow!! THIS IS so GRT & good :-). A, B, and I are amazed.\")\n",
                "print(tokenized_sequence)\n",
                "\n",
                "tokenized_sequence = tokenizer.tokenize(\"Hi, who are you? I am Optimus Prime.\")\n",
                "print(tokenized_sequence)\n",
                "\n",
                "print(\"__________________\")\n",
                "\n",
                "# Deal with pairs of sentences\n",
                "encoded_inputs = tokenizer(\"Hi, who are you?\", \"I am Optimus Prime.\", return_tensors = 'pt')\n",
                "print(encoded_inputs)\n",
                "for single_input_ids in encoded_inputs['input_ids']:\n",
                "    decoded_input = tokenizer.decode(single_input_ids)\n",
                "    print(decoded_input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'input_ids': tensor([[  101,   146,  4819, 23334,   102,     0,     0,     0,     0,     0],\n",
                        "        [  101,   146,  1567, 23334,   102,     0,     0,     0,     0,     0],\n",
                        "        [  101,  1192,  1132,  1126, 20066, 13901, 16830,  2036, 10696,   102],\n",
                        "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                        "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
                        "_________________\n",
                        "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1961, -0.2846],\n",
                        "        [ 3.0440, -4.1388],\n",
                        "        [-2.8806,  2.9183],\n",
                        "        [ 1.7171, -2.0363]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
                        "tensor([[6.1790e-01, 3.8210e-01],\n",
                        "        [9.9924e-01, 7.5892e-04],\n",
                        "        [3.0217e-03, 9.9698e-01],\n",
                        "        [9.7710e-01, 2.2901e-02]], grad_fn=<SoftmaxBackward0>)\n",
                        "_________________\n",
                        "tensor([[0.5270],\n",
                        "        [0.5146],\n",
                        "        [0.6735],\n",
                        "        [0.5164]], grad_fn=<SigmoidBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "# Brad's example\n",
                "from transformers import AutoTokenizer\n",
                "from transformers import AutoModelForSequenceClassification\n",
                "from torch.nn.functional import softmax\n",
                "from torch import sigmoid\n",
                "\n",
                "list_of_comments = ['I hate Python', 'I love Python', 'You are an ABSOLUTE idiot', '']\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
                "test_encodings = tokenizer(list_of_comments, truncation=True, padding=True, return_tensors = 'pt')\n",
                "print(test_encodings)\n",
                "\n",
                "print(\"_________________\")\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(os.path.join(NLP_MODELS_FOLDER,'tox_classifier'), num_labels = 2)\n",
                "result = model(**test_encodings)\n",
                "print(result)\n",
                "result_softmax = softmax(result.logits, dim=-1) # (not toxic, toxic)\n",
                "print(result_softmax)\n",
                "\n",
                "print(\"_________________\")\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(os.path.join(NLP_MODELS_FOLDER,'tox_regression'), num_labels=1)\n",
                "result = model(**test_encodings)\n",
                "result_sigmoid = sigmoid(result.logits)\n",
                "print(result_sigmoid) # higher indicates toxicity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'label': 'POSITIVE', 'score': 0.9994511008262634}]"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Pipelines\n",
                "from transformers import pipeline\n",
                "\n",
                "classifier = pipeline(\"sentiment-analysis\")\n",
                "result = classifier(\"I feel like you are doing an acceptable job.\")\n",
                "result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'input_ids': tensor([[  101,  2182,  2003,  1037,  7615,   102,     0],\n",
                        "        [  101,  1045,  2293, 18750,   102,     0,     0],\n",
                        "        [  101,  5410, 22868,  3084,  2033,  6517,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0],\n",
                        "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n",
                        "        [1, 1, 1, 1, 1, 0, 0],\n",
                        "        [1, 1, 1, 1, 1, 1, 1]])}\n",
                        "_________________\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
                        "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2683, -0.0348],\n",
                        "        [-0.3979, -0.1225],\n",
                        "        [-0.3856, -0.1495]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
                    ]
                }
            ],
            "source": [
                "# Custom model\n",
                "from transformers import BertTokenizer, BertForSequenceClassification\n",
                "\n",
                "inputs = ['here is a comment', 'I love Python', 'weak typing makes me sad']\n",
                "\n",
                "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "tokenized_inputs = tokenizer(inputs, truncation=True, padding=True, return_tensors = 'pt')\n",
                "print(tokenized_inputs)\n",
                "\n",
                "print(\"_________________\")\n",
                "\n",
                "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
                "outputs = model(**tokenized_inputs)\n",
                "print(outputs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Have I Been Pwned"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[8716868, 8756695, 8757964, 8758490, 8822102, 8860772, 8862943, 8865325, 8882593, 8889408, 8890205, 8932637, 8933240, 8955639, 8980383, 8981958, 9036815, 9058850, 9059340, 9060210, 9060212, 9061248, 9086764, 9092212, 9122017, 9148994, 9177112, 9195200, 9197798, 9220664, 9226749, 9227546, 9228893, 9249343, 9255687, 9257335, 9276650, 9341404, 9349998, 9350637, 9406701, 9406760, 9408095, 9447860, 9483238, 9517818, 9517870, 9546186, 9554304, 9627014, 9627571, 9629918, 9632476, 9633532, 9657695, 9663098, 9666054, 9692409, 9694406, 9695764, 9725733, 9725639, 9729051, 9760565, 9760568, 9805904, 9849074, 9883134, 9884404, 9887444, 9889083, 9891141, 9927053, 9930911, 9933872, 9978572, 9982083, 9981567, 10015487, 10019171, 10021559, 10106657, 10107248, 10112327, 10113121, 10150490, 10153221, 10193197, 10234926, 10236603, 10237774, 10289357, 10292594, 10391350, 10447722, 10502240, 10545352, 10545671, 10549182, 10551719, 10590470, 10591448, 10649776, 10650446, 10709895, 10711066, 10712690, 10770922, 10826870, 10898205, 10900211, 10900867, 10903436, 10907539, 10998698, 11002255, 11007407, 11065063, 11064875, 11068983, 11133070, 11138559, 11141234, 11209340, 11218991, 11278774, 11280490, 11281312, 11282184, 11284353, 11345420, 11353592, 11354336, 11403792, 11494105, 11498862, 11500168, 11500239, 11567427, 11646152, 11726261, 11726529, 11729897, 11730345, 11732893, 11786259, 11892521, 11972787, 11974406, 11976336, 11977257, 12142853, 12144069, 12245474, 12341830, 12344070, 12349479, 12349220, 12350347, 12455850, 12526492, 12529186, 12530398, 12530925, 12646556, 12757632, 12758994, 12759918, 12865243, 12867619, 12868502, 12870106]\n",
                        "[]\n"
                    ]
                }
            ],
            "source": [
                "## Get emails in the DB which are None, and those that are neither string nor None\n",
                "objs = sess.query(sch.Entity).where(sch.Entity.type == sch.EntityTypeEnum.person)\n",
                "not_str = []\n",
                "not_str_not_none = []\n",
                "for obj in objs:\n",
                "    if type(obj.attrs['email']) != str:\n",
                "        if obj.attrs['email'] is None:\n",
                "            not_str.append(obj.id)\n",
                "        else:\n",
                "            not_str_not_none.append(obj.id)\n",
                "print(not_str)\n",
                "print(not_str_not_none)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "b582a18c25113efe951eebbf74441ddb9a7b23854075204157e65469fc329733"
        },
        "kernelspec": {
            "display_name": "Python 3.8.2 64-bit ('py38': venv)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
